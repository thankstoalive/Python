{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "050afe1e3e3665a6a7374ab856e551ab3cd67fa5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import keras\n",
    "from keras.layers import Dense, GlobalAveragePooling1D, Embedding\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "10075545c8d8203a6a30598e01b7e4836a7cf281"
   },
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "train = pd.read_csv('./train/train.csv')\n",
    "test = pd.read_csv('./test/test.csv')\n",
    "\n",
    "# 결측치 처리\n",
    "train['Description'] = train.Description.fillna(\"none\").values\n",
    "test['Description'] = test.Description.fillna(\"none\").values\n",
    "\n",
    "# y 원핫인코딩\n",
    "target = train['AdoptionSpeed']\n",
    "y = to_categorical(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "7beeed0f81216d2f173c699dab4dda31999045f5"
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 100\n",
    "MAX_NB_WORDS = 50000 #200000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "f85c542c3fb36134cc4559339e66c9b4c84e887c"
   },
   "outputs": [],
   "source": [
    "# function to clean data\n",
    "import string\n",
    "import itertools \n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def cleanData(text, lowercase = False, remove_stops = False, stemming = False, lemmatization = False):\n",
    "    txt = str(text)\n",
    "    \n",
    "    # Replace apostrophes with standard lexicons\n",
    "    txt = txt.replace(\"isn't\", \"is not\")\n",
    "    txt = txt.replace(\"aren't\", \"are not\")\n",
    "    txt = txt.replace(\"ain't\", \"am not\")\n",
    "    txt = txt.replace(\"won't\", \"will not\")\n",
    "    txt = txt.replace(\"didn't\", \"did not\")\n",
    "    txt = txt.replace(\"shan't\", \"shall not\")\n",
    "    txt = txt.replace(\"haven't\", \"have not\")\n",
    "    txt = txt.replace(\"hadn't\", \"had not\")\n",
    "    txt = txt.replace(\"hasn't\", \"has not\")\n",
    "    txt = txt.replace(\"don't\", \"do not\")\n",
    "    txt = txt.replace(\"wasn't\", \"was not\")\n",
    "    txt = txt.replace(\"weren't\", \"were not\")\n",
    "    txt = txt.replace(\"doesn't\", \"does not\")\n",
    "    txt = txt.replace(\"'s\", \" is\")\n",
    "    txt = txt.replace(\"'re\", \" are\")\n",
    "    txt = txt.replace(\"'m\", \" am\")\n",
    "    txt = txt.replace(\"'d\", \" would\")\n",
    "    txt = txt.replace(\"'ll\", \" will\")\n",
    "    txt = txt.replace(\"--th\", \" \")\n",
    "    \n",
    "    # More cleaning\n",
    "    txt = re.sub(r\"alot\", \"a lot\", txt)\n",
    "    txt = re.sub(r\"what's\", \"\", txt)\n",
    "    txt = re.sub(r\"What's\", \"\", txt)\n",
    "    \n",
    "    \n",
    "    # Remove urls and emails\n",
    "    txt = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', ' ', txt, flags=re.MULTILINE)\n",
    "    txt = re.sub(r'[\\w\\.-]+@[\\w\\.-]+', ' ', txt, flags=re.MULTILINE)\n",
    "    \n",
    "    # Replace words like sooooooo with so\n",
    "    txt = ''.join(''.join(s)[:2] for _, s in itertools.groupby(txt))\n",
    "    \n",
    "    # Remove punctuation from text\n",
    "    txt = ''.join([c for c in text if c not in punctuation])\n",
    "    \n",
    "    # Remove all symbols\n",
    "    txt = re.sub(r'[^A-Za-z\\s]',r' ',txt)\n",
    "    txt = re.sub(r'\\n',r' ',txt)\n",
    "    \n",
    "    if lowercase:\n",
    "        txt = \" \".join([w.lower() for w in txt.split()])\n",
    "        \n",
    "    if remove_stops:\n",
    "        txt = \" \".join([w for w in txt.split() if w not in stop_words])\n",
    "        \n",
    "    if stemming:\n",
    "        st = PorterStemmer()\n",
    "        txt = \" \".join([st.stem(w) for w in txt.split()])\n",
    "    \n",
    "    if lemmatization:\n",
    "        wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        txt = \" \".join([wordnet_lemmatizer.lemmatize(w, pos='v') for w in txt.split()])\n",
    "\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "f04bcc158f5828a133d033d9394827a30809056e"
   },
   "outputs": [],
   "source": [
    "# clean comments\n",
    "train['Description'] = train['Description'].map(lambda x: cleanData(x, lowercase=True, remove_stops=True, stemming=False, lemmatization = False))\n",
    "test['Description'] = test['Description'].map(lambda x: cleanData(x, lowercase=True, remove_stops=True, stemming=False, lemmatization = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "2ade4daecf48aadea47ad8c86d5c8e11e9ee780f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train data tensor: (14993, 100)\n",
      "# of words :24,752\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(lower=False, filters='',num_words = MAX_NB_WORDS) # 설정\n",
    "tokenizer.fit_on_texts(train['Description']) # token 번호 지정?\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(train['Description']) # train : text -> sequence\n",
    "test_sequences = tokenizer.texts_to_sequences(test['Description']) # test : text -> sequence\n",
    "\n",
    "train_data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH) # train : padding why? 길이 맞출려고\n",
    "\n",
    "print('Shape of train data tensor:', train_data.shape)\n",
    "\n",
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH) # test : padding\n",
    "\n",
    "print(\"max sequence length : {:,}\" .format(MAX_SEQUENCE_LENGTH))\n",
    "nb_words = (np.max(train_data) + 1)\n",
    "print(\"# of words :{:,}\" .format(nb_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "16fdd99512e99f2bd72d09a25275e28df69fa6ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 100, 50)           1237600   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_2 ( (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 255       \n",
      "=================================================================\n",
      "Total params: 1,237,855\n",
      "Trainable params: 1,237,855\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "node_n = 32\n",
    "\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "model = Sequential()\n",
    "model.add(Embedding(nb_words,50,input_length=MAX_SEQUENCE_LENGTH))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "# model.add(Dense(node_n, activation='relu'))\n",
    "# model.add(Dense(node_n, activation='relu'))\n",
    "# model.add(Dense(node_n, activation='relu'))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "ea8a9b87accc3e13131df5f28bc88068690a535e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11994 samples, validate on 2999 samples\n",
      "Epoch 1/100\n",
      "11994/11994 [==============================] - 3s 229us/step - loss: 1.5439 - acc: 0.2698 - val_loss: 1.4932 - val_acc: 0.2701\n",
      "Epoch 2/100\n",
      "11994/11994 [==============================] - 2s 200us/step - loss: 1.4724 - acc: 0.3179 - val_loss: 1.4629 - val_acc: 0.3328\n",
      "Epoch 3/100\n",
      "11994/11994 [==============================] - 2s 197us/step - loss: 1.4557 - acc: 0.3368 - val_loss: 1.4566 - val_acc: 0.3224\n",
      "Epoch 4/100\n",
      "11994/11994 [==============================] - 2s 196us/step - loss: 1.4465 - acc: 0.3467 - val_loss: 1.4508 - val_acc: 0.3254\n",
      "Epoch 5/100\n",
      "11994/11994 [==============================] - 2s 195us/step - loss: 1.4362 - acc: 0.3643 - val_loss: 1.4449 - val_acc: 0.3341\n",
      "Epoch 6/100\n",
      "11994/11994 [==============================] - 2s 194us/step - loss: 1.4238 - acc: 0.3816 - val_loss: 1.4399 - val_acc: 0.3408\n",
      "Epoch 7/100\n",
      "11994/11994 [==============================] - 2s 193us/step - loss: 1.4100 - acc: 0.3970 - val_loss: 1.4328 - val_acc: 0.3348\n",
      "Epoch 8/100\n",
      "11994/11994 [==============================] - 2s 195us/step - loss: 1.3935 - acc: 0.4098 - val_loss: 1.4251 - val_acc: 0.3394\n",
      "Epoch 9/100\n",
      "11994/11994 [==============================] - 2s 194us/step - loss: 1.3759 - acc: 0.4289 - val_loss: 1.4187 - val_acc: 0.3498\n",
      "Epoch 10/100\n",
      "11994/11994 [==============================] - 2s 196us/step - loss: 1.3562 - acc: 0.4518 - val_loss: 1.4130 - val_acc: 0.3461\n",
      "Epoch 11/100\n",
      "11994/11994 [==============================] - 2s 194us/step - loss: 1.3357 - acc: 0.4629 - val_loss: 1.4066 - val_acc: 0.3535\n",
      "Epoch 12/100\n",
      "11994/11994 [==============================] - 2s 198us/step - loss: 1.3141 - acc: 0.4841 - val_loss: 1.4005 - val_acc: 0.3575\n",
      "Epoch 13/100\n",
      "11994/11994 [==============================] - 2s 199us/step - loss: 1.2917 - acc: 0.4997 - val_loss: 1.3967 - val_acc: 0.3615\n",
      "Epoch 14/100\n",
      "11994/11994 [==============================] - 2s 196us/step - loss: 1.2681 - acc: 0.5141 - val_loss: 1.3921 - val_acc: 0.3605\n",
      "Epoch 15/100\n",
      "11994/11994 [==============================] - 2s 196us/step - loss: 1.2450 - acc: 0.5338 - val_loss: 1.3891 - val_acc: 0.3705\n",
      "Epoch 16/100\n",
      "11994/11994 [==============================] - 2s 197us/step - loss: 1.2211 - acc: 0.5476 - val_loss: 1.3872 - val_acc: 0.3788\n",
      "Epoch 17/100\n",
      "11994/11994 [==============================] - 2s 198us/step - loss: 1.1974 - acc: 0.5582 - val_loss: 1.3818 - val_acc: 0.3781\n",
      "Epoch 18/100\n",
      "11994/11994 [==============================] - 2s 196us/step - loss: 1.1736 - acc: 0.5679 - val_loss: 1.3804 - val_acc: 0.3811\n",
      "Epoch 19/100\n",
      "11994/11994 [==============================] - 2s 196us/step - loss: 1.1505 - acc: 0.5838 - val_loss: 1.3816 - val_acc: 0.3805\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2168c5ac320>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(patience = 1)\n",
    "\n",
    "model.fit(train_data, y, validation_split=0.2, nb_epoch=100, batch_size=128, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02822204, 0.17540796, 0.2717901 , 0.25085285, 0.27372706],\n",
       "       [0.02997983, 0.1812263 , 0.26700017, 0.20136054, 0.3204331 ],\n",
       "       [0.0257531 , 0.14775589, 0.24293004, 0.20147549, 0.38208547],\n",
       "       ...,\n",
       "       [0.04051525, 0.30515024, 0.16184403, 0.21062815, 0.28186235],\n",
       "       [0.03428663, 0.16891043, 0.22220442, 0.23371549, 0.3408831 ],\n",
       "       [0.02166584, 0.13252798, 0.17451857, 0.12695326, 0.54433435]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model.predict(test_data)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 4, 4, ..., 1, 4, 4], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = pred.argmax(axis=-1)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    2035\n",
       "2    1122\n",
       "3     462\n",
       "1     329\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(pred).value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
